Istio 权威指南



### 服务治理

微服务的服务治理指的是服务注册、服务发现、负载均衡、熔断和调用链追踪等。这些主要是解决单体转微服务过程中带来的问题。

#### 服务治理的形态

##### 第一种形态：治理逻辑和业务代码耦合

需要开发者写代码实现服务注册、服务发现、负载均衡等逻辑，这会导致代码中存在大量的重复代码且随着治理逻辑的升级需要协同升级。

##### 第二种形态：治理逻辑和业务代码解耦

把治理逻辑抽象成一个公共库，让所有微服务通过这个公共库来自然获得治理能力（典型例子就是 Spring Cloud）。这种模式虽然解耦了治理逻辑和业务代码，但是实际上治理逻辑还是会与业务代码一起发布，且当治理逻辑发生变动时也需要重新编译以及部署业务代码，会给生产带来额外的风险。

##### 第三种形态：治理逻辑与业务进程解耦

将治理逻辑彻底从用户的业务进程中剥离出来（也就是 sidecar 模式）。治理逻辑与业务代码都以独立的进程存在，业务与治理也可以独立进行升级，对应用的侵入也最小。这种模式也就是所谓的**服务网格**形态。

#### Istio 所解决的问题

微服务架构只是一种架构风格，是一种敏捷的软件工程实践。而 Istio 是用来解决应用访问中各种实际问题的，**并不只是微服务模式的另一种落地形式**。

> 通俗的理解，就算不是微服务架构的软件，也可以使用 Istio。

#### 服务网格

William Moran关于服务网格（S ervice Mesh）的定义：

* 基础设施：服务网格是一种处理服务间通信的基础设施层
* 云原生：服务网格尤其适用于在云原生场景中帮助应用程序在复杂的服务拓扑间可靠地传递请求
* 网络代理：在实际应用中，服务网格一般通过一组轻量级网络代理执行治理逻辑
* 对应用透明：虽然轻量级网络代理会和应用程序部署在一起，但应用感知不到代理的存在，仍通过原本的方式工作

云原生时代，服务量剧增，应用间的拓扑也更加复杂，治理需求也越来越强烈。迫切的需要一种具备云原生的动态、弹性特点的应用治理基础设施。

##### Sidecar

Sidecar 与应用的解耦带来的完全无侵入、开发语言无关的特点极大地降低了应用开发者的开发成本。另外作为透明代理，Sidecar 不影响原本的业务访问模式，透明代理会拦截应用程序的所有 Inbound 和 Outbound 流量并对服务运行时进行更高级的控制，使服务可监控、可管理。

Sidecar 是服务网格动作的执行体，全局的管理规则和服务网格内的元数据维护通过一个统一的控制面进行。应用程序感知不到透明代理，也就不会和控制面存在任何联系。

但是 Sidecar 模式在访问链路中多引入的两跳也是不容回避的问题。这会导致两处额外的延迟（可通过基于 **eBPF** 提升流量拦截的效率）和可能的故障点，这也为系统的访问性能、整体可靠性及系统的复杂度都带来了新的挑战。

##### Envoy

Istio 配套的数据面 Envoy 已经是当前服务网格数据面的标准实现，其控制面的 xDS 协议也已经成为服务网格数据面的事实标准协议。Envoy 满足服务网格对透明代理轻量、高性能的要求，提供 L3/L4 过滤器、HTTP L7 过滤器，支持 HTTP、gRPC、TCP、TLS、MySQL、Redis 等多种协议。

#### Kubernetes

Kubernetes 虽然已经通过 Service 机制提供了服务注册、服务发现以及负载均衡问题，通过 Readiness 健康检查实现了一定程度的故障隔离和恢复功能。但是 Kubernetes 提供的这些能力大都停留在 L4 的互访上，对于服务化要求较高的用户，如果需要实现熔断、限流、动态路由或者调用埋点等应用层的能力，仍然需要使用服务网格来提供可观测性、服务韧性、灵活分流、弹性、安全等能力。

Istio 正是在 Kubernetes 之上叠加了一层面向应用、感知应用的服务管理平台和基础设施，提供了强大的可扩展的非侵入式的七层流量管理能力。

##### Istio 的管理

Istio 在管理 Kubernetes 流量时，复用了 Kubernetes 中 Service 的定义，它的服务发现是从 Kube-Apiserver 中获取 Service 和 Endpoints，然后将其转换为 Istio 的服务模型。但是其数据面组件不再是 kube-proxy，而是在每个 Pod 中部署的 sidecar 容器，也可以将其看作每个服务实例的 Proxy。

当发生服务间访问时，Istio 数据面透明地拦截 Pod 的 Inbound 流量和 Outbound 流量，接管服务间访问的流量。服务网格数据面代理识别和解析各种应用层协议，根据控制面上该服务对应协议的配置执行丰富的流量管理动作。

基于 sidecar 的服务网格透明代理代替了 Kubernetes 中的 kube-proxy 拦截和处理东西流量。对于入口的南北流量，Istio 也有基于 Envoy 建立的 ingress-gateway。

##### Istio 如何发挥作用

Istio 最大化利用了 Kubernetes 这个基础设施，主要体现在以下方面：

* **基于 Kubernetes Pod 的服务网格数据面管理**：基于 Kubernetes 一个 Pod 多个容器的优秀设计使得部署运维对用户透明。用户还是用原本的方式创建负载，通过 Istio 的自动注入机制，在业务 Pod 创建时自动给指定的负载注入代理容器。
* **基于 Kubernetes 机制的透明流量拦截**：早期基于 init container 机制，在业务容器启动前执行 iptables 规则，将业务容器的出入流量拦截到数据面代理上。新版本基于 Kubernetes CNI 机制，在 Pod 创建或销毁时执行服务网格拦截流量的规则，将业务流量转发到服务网格数据面代理上。
* **基于 Kubernetes Service 的服务发现**：Istio 的服务发现机制完美地基于 Kubernetes 的域名访问机制构建而成。
* **基于 Kubernetes 的控制面组件管理**：Istio 的控制面组件是以容器形式部署在 Kubernetes 集群中，这些 Kubernetes 原生或扩展的能力，大大方便了服务网格组件自身的运维。

### Istio 的架构

Istio 在架构上分为控制面和数据面两部分，**控制面只有一个单体应用 istiod**，**数据面主要由伴随每个业务程序部署的代理 Envoy 组成**，Envoy 根据控制面的配置执行流量管理操作。

#### 服务模型

服务、服务版本、服务实例等对象构成了 Istio 的服务模型。在 Kubernetes 场景中，Istio 的服务模型基于 Kubernetes 的 Service、Endpoints 资源对象构建而成，并加上部分约束来满足 Istio 服务模型的要求。

* 端口命名格式：**规范格式是 `<protocol>[-suffix]`** ，其中 `protocol` 可以是 `tcp, http, grpc, mysql, redis` 等。Istio 根据端口名获取应用协议的类型，进而提供对应的流量治理能力。在 `k8s 1.18` 之后的版本可以通过 `appProtocol` 配置服务的应用协议。
* 服务关联：**一个服务相关联的 Pod 在同一个端口上最好是相同的协议**，否则 Envy 监听器可能会出现意料之外的错误。
* 应用的 Deployment 使用 app 和 version 标签。基于 app 和 version 标签构建的可观测性元数据信息可以对可观测性进行各种维度的管理，version 标签还可用于灰度发布中对灰度版本进行标识。

#### 服务 Service

服务是服务网格 Istio 管理的主要资源对象。在 Kubernetes 场景中，满足 Istio 规范约束的服务比较简单，**唯一需要做的就是在端口名称上指定协议类型**。

#### 服务版本 Version

在 Istio 中，灰度发布是一个重要的场景，要求一个服务有多个不同的版本的实现。在 Kubernetes 中对多个版本的定义是**将一个服务关联到多个 Deployment，每个 Deployment 都对应服务的一个版本**（通过 app 和 version 标签）。

#### 服务实例 Instance

服务实例是真正处理服务请求的后端。在 Kubernetes 场景中对应的是 Endpoints 资源。Istio 对虚拟机和其他形态的服务和负载也提供一套更通用的服务模型，在虚拟机场景中，Istio 提供了 `ServiceEntry`、`WorkloadEntry`、`WorkloadGroup` 等对象来描述。

### 主要组件

控制面组件原本使用的微服务模式，有 `Pilot`、`Galley`、`Citadel` 和 `Mixer` 组成，出于简化安装复杂度、配置复杂度和简化控制面维护等原因，将其重构为一个单体应用 istiod。在 istio 内部，依然在使用组件化进行维护

#### Pilot

Pilot 是 istio 的控制中枢，用于下发指令控制 Envoy 完成一系列功能。Pilot 主要包含两部分工作：服务发现和配置管理。除了服务发现之外，Pilot 更重要的功能是构造维护和向数据面下发规则，包括 `VirtualService`、`DestinationRule`、`Gateway` 等流量规则，也包括 `RequestAuthentication`、`PeerAuthentication` 等安全规则。

#### Citadel

Citadel 是 Istio 的核心安全组件，提供了自动生成、分发、轮换与撤销密钥和证书的功能。

#### Galley

Galley 是服务网格控制面负责配置管理的组件，其作为 Webhook Server 也承担了 Kubernetes 的准入控制器，在创建资源对象时对其进行校验、拦截等操作。同时 Galley 还负责监控各种 API 资源对象，在资源发生变化时，通知 Pilot 根据最新的对象生成 xDS 配置，并推送给相关的数据面 sidecar。

### 数据面组件

数据面组件一般特指数据面代理 Istio-proxy，这不是通用的 Envoy，而是叠加了 Istio 的 Proxy 功能的一个扩展版本，包括可观测性元数据的交换，指标生成等

#### Ambient

Istio 在 2022 年 9 月推出了另一种共享代理的数据面模式 Ambient，**在不使用传统 sidecar 的情况下，保持零信任安全、流量管理和可观测性等核心功能**。但相比 sidecar 模式，其业务侵入性更低，升级管理更简单。

Ambient 代理模型包含两个组件：ztunnel 和 waypoint。ztunnel 以 Daemonset 部署在每个节点上，为服务网格中的应用提供 mTLS、可观测性、身份验证和四层授权等功能。waypoint 则专注于七层治理的能力，包括 HTTP 路由、负载均衡、熔断、重试等流量管理功能以及七层授权。

### Istio 的实现

#### 流量拦截

**Istio 默认通过注入 init 容器进行 iptables 规则的设置**，拦截进入容器的 Inbound 流量和从应用程序发出的 Outbound 流量。这种方案实现简单，但是**需要 `NET_ADMIN` 和 `NET_RAW` 权限执行网络设置**。而使用 **Istio CNI 插件可以取代 Init 容器进行流量拦截规则的设置**，插件通过识别用户应用程序 Pod 和需要流量重定向的 sidecar，并**在 Kubernetes Pod 生命周期的网络设置阶段进行设置**，从而消除用户在启用 Istio 的集群中对 Pod 额外的权限要求。

#### 服务发现

服务发起方的 Envoy 调用 Istiod 的服务发现接口获取目标服务的实例列表，为负载均衡做准备。

##### 负载均衡

服务发起方的 Envoy 根据配置的负载均衡策略选择服务实例，并将请求分发到对应的目标服务实例上。

#### 流量治理

Envoy 从 Istiod 中获取配置的流量治理规则，在拦截到 Inbound 流量和 Outbound 流量时执行治理逻辑。

#### 访问安全

在服务间访问时，Envoy 可以进行双向认证和通道加密，并基于授权策略的配置对请求进行鉴权。

#### 服务监控

在服务通信时，通信双方的 Envoy 根据请求的信息进行监控指标的统计、访问日志的收集和分布式调用链的埋点等。

### 流量治理的原理

服务治理要解决的问题包括负载均衡、服务熔断、故障注入、灰度发布、故障转移、入口流量以及出口流量管理等。在 Istio 中实现这些功能时无需修改任何应用的代码。相比于微服务的 SDK 方式，Istio 以一种更轻便、透明的方式向用户提供这些功能。Istio 流量治理的目标：以基础设施的方式向用户提供各种非侵入式的流量治理能力，用户只需要关注自己的业务逻辑开发，无需关注通用的服务治理能力。

#### 负载均衡

负载均衡时在服务调用方使用一个服务名发起访问时能找到一个合适的后端，把流量分发过去。传统的负载均衡器一般是在服务端提供的，在微服务化之后一般是采用客户端进行负载均衡。

在 Istio 中，数据面代理 Envoy 执行负载均衡策略，控制面 Istiod 负责维护服务发现数据。**Istio 支持轮询、随机和最小连接数等负载均衡算法**。在 Kubernetes 中的 Service 默认通过**轮询**的方式把 Service 的访问分发到后端实例 Pod，在负载均衡机制上与 Istio 不同的是，Kube-proxy 只能解析四层流量，因此**只能支持四层负载均衡**。

#### 服务熔断

韧性是系统设计的一个非常核心的考虑因素。对于一个系统，预防失败是一方面，更重要的是接受失败，**在失败时保证业务影响最小，特别是对核心业务影响小，并尽快从失败中恢复业务**。在微服务场景中，对系统的韧性要求体现的更加明显，局部访问经常影响整个系统，进而影响最终业务。为了解决这个问题，我们需要让故障服务快速失败，让上层的调用方感知到所依赖的服务出现问题，并立即进行故障处理。

熔断器时提高微服务韧性的非常典型的手段，它最典型的应用场景时防止网络和服务调用故障级联发生，限制故障的影响范围，防止故障蔓延到纸系统整体性能下降或雪崩。

> [Circuit Breaker](https://martinfowler.com/bliki/CircuitBreaker.html) by Martin Fowler.

在 Istio 中服务熔断被拆分成连接池管理（`ConnectionPoolSetting`）和异常点检查（`OutlierDetection`）这两种配置：

* 通过限制某个客户端对目标服务的连接数、访问请求数等，避免对一个服务的过量访问，如果超出配置的阈值，则快速断路请求。
* 如果某个服务在考察的时间段内连续异常的次数超过阈值，则将该实例隔离，避免影响整个系统。

Istio 提供的异常检查点机制**动态地将异常实例从负载均衡池中移除**，保证了服务的总体访问成功率。被移除的实例在一段时间之后，还会被加回来再次尝试访问，如果成功则认为实例正常；如果不成功，则认为实例不正常，重新逐出，后面驱逐的时间等于一个基础时间乘以驱逐的次数。另外在 Istio 中可以控制驱逐比例，当有太多的实例被移除时，就会进入恐慌模式，会忽略负载均衡池上实例的健康标记，仍然会向所有的实例发送请求，保证一个服务的整体可用性。

#### 故障注入

为了提高系统韧性，必要的故障模拟测试也被证明为积极有效的。故障注入是一种评估系统可靠性的有效方法，在实践上使用一种手段故意在待测试的系统中引入故障，测试系统的健壮性和应对故障的能力，例如异常处理、故障恢复等。

故障注入从方法上来说有编译期故障注入和运行期故障注入，**Istio 中的故障注入采用的是在网络协议栈中注入对应协议的故障，干预服务间的调用，不用修改业务代码**（例如对于 HTTP 类型的请求，我们在响应中注入一个指定的 HTTP 错误码）。还可以给特定的服务注入一个固定的延迟，这样客户端看到的就跟服务端真的响应慢一样。

#### 灰度发布

在新版本上线时，我们直接将老版本全部升级到是非常有风险的，一般的做法是，新老版本同时在线，新版本只切分少量流量出来，在确认新版本没有问题后，在逐步扩大流量比例，这正是灰度发布需要解决的问题。常见的场景有：

* 蓝绿发布：**让新版本部署在另一套独立的资源上，在新版本可用后将所有流量从旧版本切到新版本**。如果新版本工作正常，则删除老版本；如果新版本工作异常，则快速切换到老版本。
* A/B 测试：**同时部署 A 和 B 两个对等的版本来接收流量**，按一定的目标选取策略，将部分用户的流量分发到 A 和 B 版本上，通过分析数据来评估哪个版本更符合业务目标，并最终选择采用该版本。
* 金丝雀发布：**上线一个新版本，从老版本切分一部分线上流量到新版本**，判定新版本在生产环境下的实际表现。在观察和评估新版本没有问题后再增加切分的比例，直到全部流量切分到新版本，这是一个渐变、尝试的过程。

上述几种场景核心是**支持对流量进行切分**，能否提供灵活的流量策略是判断基础设施灰度发布能力的重要指标。

##### 灰度发布方式

灰度发布在技术上的核心要求是提供一种机制来满足多个不同版本同时在线，并灵活配置规则给这些版本分配流量。常见的方式有：

* 基于负载均衡的灰度发布：在入口的负载均衡器上配置流量策略
* 基于 Kubernetes 的灰度发布：通过控制 Pod 的比例来实现，依靠 kube-proxy 把流量均衡的分发给目标后端
* 基于 Istio 的灰度发布：通过服务网格数据面代理解析应用协议，执行控制面配置的分流规则，在不同版本间进行灵活的流量切分。Istio 还支持基于请求内容的灰度策略，例如通过匹配请求头的内容来将请求分发给不同的版本。

#### 故障转移

故障转移是系统韧性设计中的一个基础能力，一般表示在服务器、系统硬件或网络发生故障不能服务时切换到备用的系统，自动无缝完成故障检测和切换，减少或消除对使用方或最终用户的影响，从而提高整个系统对外的可用性。在微服务中虽然有负载均衡机制来完成部分的故障转移，但是可以在 Zone、Region 尺度上做更进一步的转移。

Istio 可以**以非侵入的方式实现以上的故障转移过程**，在控制面配置故障转移策略，服务网格数据面拦截到源服务对目标服务的请求时，在转发请求的过程中根据故障转移策略将流量转移到配置的服务实例上。

在通常的故障转移过程中，需要通过心跳等机制对主备服务实例持续进行健康检查，当检查到主服务实例不健康时，出发转移流程，由备用服务实例接管主服务实例的工作。**在 Istio 中采用了一套更简单的被动健康机制，先观察服务实例被访问的情况，再通过实际访问数据判定服务实例的健康状态，最后根据配置的策略进行故障转移**。

#### 入口流量

从网络或服务访问的角度来看，在集群入口一般需要提供入口网关将外部流量导入集群。网关内外一般术语不同的安全信任域，网关要对入口流量进行严格管理，包括代理内部服务做证书认证或 JWT 认证，并在入口做访问授权管理。

##### Kubernets 的入口流量管理

Kubernetes 通过提供 LoadBanancer 和 Ingress 对象来处理入口流量问题。Ingress 作为流量入口，根据七层协议中的域名以及路径将流量导向不同的服务后端。Kubernets 提供的这两种方式主要还是解决外部流量接入问题，不涉及管理。

##### 传统微服务的入口流量

传统微服务通过网关服务来实现对入口流量的管理，网关通过断言对匹配的请求进行路由，通过过滤器执行流量治理操作。

##### Istio 的入口流量

在 Istio 中是通过 Ingress-gateway 除了提供类似 Ingress 的七层流量接入，**还提供了更多的高级流量治理功能**，包括对匹配流量的重试、重定向、超时、头域操作等，还可以对目标服务进行熔断、限流、分流、镜像，并提供丰富的可观测性和认证授权等安全功能。

对比 Ingress 或者 Api-Gateway 服务，**基于 istio 的入口流量管理会对来自集群内部或外部的流量执行统一的策略**。也就是一套规则同时对内部、外部流量生效。

#### 出口流量

在实现一个完整的业务功能时，全部依靠内部服务经常无法支撑整个系统，也需要搭配若干外部的中间件才能完成该功能。一般与内部服务间的访问相比，对外部服务的访问在大多数时候会有更强的管理需求。

##### Istio 的出口流量

在 Istio 中，只要外部服务通过一种机制注册到服务网格中，成为服务网格管理对象的一部分，即可像对服务网格内部的普通服务一样，对服务网格外部服务的访问进行管理，进而实现大多数服务网格治理功能。

Istio 专门提供了 Egress-gateway 管理服务网格的出口流量，在访问外部服务时，通过服务网格内部的 sidecar 即可执行治理功能。

### 流量治理配置

Istio 通过提供一系列服务管理配置来实现流量治理功能。

#### VirtualService

